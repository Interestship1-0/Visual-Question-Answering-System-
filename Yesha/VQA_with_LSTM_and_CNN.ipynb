{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VQA_with_LSTM_and_CNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "22Hm1w7kS3t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/VQA_Demo-master')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcti_Xao5mHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import os, argparse\n",
        "import cv2, spacy, numpy as np\n",
        "from keras.models import model_from_json\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.externals import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I88HtLlNZ9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4F2iclZ5vUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# File paths for the model, all of these except the CNN Weights are \n",
        "# provided in the repo, See the models/CNN/README.md to download VGG weights\n",
        "VQA_model_file_name      = '/content/VQA_Demo-master/My Drive/VQA_Demo-master/VQA_Demo-master/models/VQA/VQA_MODEL.json'\n",
        "VQA_weights_file_name   = '/content/VQA_Demo-master/My Drive/VQA_Demo-master/VQA_Demo-master/models/VQA/VQA_MODEL_WEIGHTS.hdf5'\n",
        "label_encoder_file_name  = '/content/VQA_Demo-master/My Drive/VQA_Demo-master/VQA_Demo-master/models/VQA/FULL_labelencoder_trainval.pkl'\n",
        "CNN_weights_file_name   = '/content/VQA_Demo-master/My Drive/VQA_Demo-master/VQA_Demo-master/models/CNN/vgg16_weights .h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZOrUqLAS7UN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/VQA_Demo-master/My Drive/VQA_Demo-master/VQA_Demo-master')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47sBEtynNZyB",
        "colab_type": "text"
      },
      "source": [
        "## Model Idea\n",
        "This uses a classical CNN-LSTM  model like shown below, where Image features and language features are computed separately and combined together and a multi-layer perceptron is trained on the combined features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYZ1DaqgNc64",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"http://i.imgur.com/Za5P1ZZ.png\">\n",
        "[Source](http://arxiv.org/pdf/1505.00468v4.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbY30V-1OCK0",
        "colab_type": "text"
      },
      "source": [
        "## Pretrained VGG Net (VGG-16)\n",
        "\n",
        "While VGG Net is not the best CNN model for image features, GoogLeNet (winner 2014) and ResNet (winner 2015) have superior classification scores, but VGG Net is very versatile, simple, relatively small and more importantly portable to use. \n",
        "\n",
        "<img src=\"http://www.robots.ox.ac.uk/~vgg/research/very_deep/images/table_ILSVRC.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FRgkXC76PbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_model(CNN_weights_file_name):\n",
        "    ''' Takes the CNN weights file, and returns the VGG model update \n",
        "    with the weights. Requires the file VGG.py inside models/CNN '''\n",
        "    from models.CNN.VGG import VGG_16\n",
        "    image_model = VGG_16(CNN_weights_file_name)\n",
        "\n",
        "    # this is standard VGG 16 without the last two layers\n",
        "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # one may experiment with \"adam\" optimizer, but the loss function for\n",
        "    # this kind of task is pretty standard\n",
        "    image_model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
        "    return image_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCw3rPMUPK4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "import tensorflow as tf\n",
        "import keras.backend.tensorflow_backend as tfback\n",
        "\n",
        "\n",
        "print(\"tf.__version__ is\", tf.__version__)\n",
        "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
        "\n",
        "def _get_available_gpus():\n",
        "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
        "\n",
        "    # Returns\n",
        "        A list of available GPU devices.\n",
        "    \"\"\"\n",
        "    #global _LOCAL_DEVICES\n",
        "    if tfback._LOCAL_DEVICES is None:\n",
        "        devices = tf.config.list_logical_devices()\n",
        "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
        "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
        "\n",
        "tfback._get_available_gpus = _get_available_gpus\n",
        "\n",
        "\n",
        "model_vgg = get_image_model(CNN_weights_file_name)\n",
        "plot_model(model_vgg, to_file='model_vgg.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWKd_3-S6dpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_features(image_file_name, CNN_weights_file_name):\n",
        "    ''' Runs the given image_file to VGG 16 model and returns the \n",
        "    weights (filters) as a 1, 4096 dimension vector '''\n",
        "    image_features = np.zeros((1, 4096))\n",
        "    # Magic_Number = 4096  > Comes from last layer of VGG Model\n",
        "\n",
        "    # Since VGG was trained as a image of 224x224, every new image\n",
        "    # is required to go through the same transformation\n",
        "    im = cv2.resize(cv2.imread(image_file_name), (224, 224))\n",
        "    im = im.transpose((2,0,1)) # convert the image to RGBA\n",
        "\n",
        "    \n",
        "    # this axis dimension is required because VGG was trained on a dimension\n",
        "    # of 1, 3, 224, 224 (first axis is for the batch size\n",
        "    # even though we are using only one image, we have to keep the dimensions consistent\n",
        "    im = np.expand_dims(im, axis=0) \n",
        "\n",
        "    image_features[0,:] = get_image_model(CNN_weights_file_name).predict(im)[0]\n",
        "    return image_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13TOKKcJBL05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_question_features(question):\n",
        "    ''' For a given question, a unicode string, returns the time series vector\n",
        "    with each word (token) transformed into a 300 dimension representation\n",
        "    calculated using Glove Vector '''\n",
        "    word_embeddings = spacy.load('en', vectors='en_vectors_web_lg')\n",
        "    tokens = word_embeddings(question)\n",
        "    question_tensor = np.zeros((1, len(tokens), 96))\n",
        "    for j in range(len(tokens)):\n",
        "            question_tensor[0,j,:] = tokens[j].vector\n",
        "    return question_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvwagjteBTx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings = spacy.load('en', vectors='en_vectors_web_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgKIWFN9BYrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obama = word_embeddings(u\"obama\")\n",
        "putin = word_embeddings(u\"putin\")\n",
        "banana = word_embeddings(u\"banana\")\n",
        "monkey = word_embeddings(u\"monkey\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvjUHiNvBg4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "1021384e-c36c-4ce6-826a-f42cf195cac1"
      },
      "source": [
        "obama.similarity(putin)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6526194124802338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fk-UrCiCxD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "ad4fb10d-4e94-4a7b-9536-cf6bd9fb19c9"
      },
      "source": [
        "obama.similarity(banana)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615127295688744"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWYb-5KWDCfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_VQA_model(VQA_model_file_name, VQA_weights_file_name):\n",
        "    ''' Given the VQA model and its weights, compiles and returns the model '''\n",
        "\n",
        "    # thanks the keras function for loading a model from JSON, this becomes\n",
        "    # very easy to understand and work. Alternative would be to load model\n",
        "    # from binary like cPickle but then model would be obfuscated to users\n",
        "    vqa_model = model_from_json(open(VQA_model_file_name).read())\n",
        "    vqa_model.load_weights(VQA_weights_file_name)\n",
        "    vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "    return vqa_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RRzxsFyTDm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_file_name = '/content/VQA_Demo-master/My Drive/VQA_Demo-master/VQA_Demo-master/test.jpg'\n",
        "question = u\"What vehicle is in the picture?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUuGSyKwOvwp",
        "colab_type": "text"
      },
      "source": [
        "# <center> What vehicle is in the picture ? </center>\n",
        "<img src=\"test.jpg\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vDVsP-iDefu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the image features\n",
        "image_features = get_image_features(image_file_name, CNN_weights_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzQvMxIBTlPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the question features\n",
        "question_features = get_question_features(question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiRTOytum6mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_vqa = get_VQA_model(VQA_model_file_name, VQA_weights_file_name)\n",
        "plot_model(model_vqa, to_file='model_vqa.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieQxPBBrbw9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_output = model_vqa.predict([question_features, image_features])\n",
        "\n",
        "# This task here is represented as a classification into a 1000 top answers\n",
        "# this means some of the answers were not part of training and thus would \n",
        "# not show up in the result.\n",
        "# These 1000 answers are stored in the sklearn Encoder class\n",
        "labelencoder = joblib.load(label_encoder_file_name)\n",
        "for label in reversed(np.argsort(y_output)[0,-5:]):\n",
        "    print(str(round(y_output[0,label]*100,2)).zfill(5), \"% \", labelencoder.inverse_transform(label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnYr4u7XRit1",
        "colab_type": "text"
      },
      "source": [
        "51.87 % train <br>\n",
        "031.5 % bicycle <br>\n",
        "03.81 % bike <br>\n",
        "02.91 % bus <br>\n",
        "02.54 % scooter <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkrOk-VpQPTy",
        "colab_type": "text"
      },
      "source": [
        "# Demo with image URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP8wnqKWQQZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_features(image_file_name):\n",
        "    ''' Runs the given image_file to VGG 16 model and returns the \n",
        "    weights (filters) as a 1, 4096 dimension vector '''\n",
        "    image_features = np.zeros((1, 4096))\n",
        "        \n",
        "    from skimage import io\n",
        "    # if you would rather not install skimage, then use cv2.VideoCapture which surprisingly can read from url\n",
        "    # see this SO answer http://answers.opencv.org/question/16385/cv2imread-a-url/?answer=16389#post-id-16389\n",
        "    im = cv2.resize(io.imread(image_file_name), (224, 224))\n",
        "    im = im.transpose((2,0,1)) # convert the image to RGBA\n",
        "\n",
        "    \n",
        "    # this axis dimension is required because VGG was trained on a dimension\n",
        "    # of 1, 3, 224, 224 (first axis is for the batch size\n",
        "    # even though we are using only one image, we have to keep the dimensions consistent\n",
        "    im = np.expand_dims(im, axis=0) \n",
        "\n",
        "    image_features[0,:] = model_vgg.predict(im)[0]\n",
        "    return image_features"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBfppp-zQZON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_file_name = \"http://www.newarkhistory.com/indparksoccerkids.jpg\"\n",
        "# get the image features\n",
        "image_features = get_image_features(image_file_name)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe_GffFvQuF1",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"http://www.newarkhistory.com/indparksoccerkids.jpg\">\n",
        " <center> What are they playing? </center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jShMESAaQvPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question = u\"What are they playing?\"\n",
        "\n",
        "# get the question features\n",
        "question_features = get_question_features(question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW_IipwpQ66I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_output = model_vqa.predict([question_features, image_features])\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "for label in reversed(np.argsort(y_output)[0,-5:]):\n",
        "    print str(round(y_output[0,label]*100,2)).zfill(5), \"% \", labelencoder.inverse_transform(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybGnVWwzRJIQ",
        "colab_type": "text"
      },
      "source": [
        "55.44 % frisbee <br>\n",
        "18.91 % tennis <br>\n",
        "16.95 % baseball <br>\n",
        "08.31 % soccer <br>\n",
        "00.07 % ball"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6ZOu6LkRxpR",
        "colab_type": "text"
      },
      "source": [
        "##Asking another question!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya8WLZjDR1Tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question = u\"Are they playing Frisbee?\"\n",
        "\n",
        "# get the question features\n",
        "question_features = get_question_features(question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nZRpEDNR8XE",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"http://www.newarkhistory.com/indparksoccerkids.jpg\">\n",
        "<center> Are they playing Frisbee? </center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzWS9NK9R-Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_output = model_vqa.predict([question_features, image_features])\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "for label in reversed(np.argsort(y_output)[0,-5:]):\n",
        "    print str(round(y_output[0,label]*100,2)).zfill(5), \"% \", labelencoder.inverse_transform(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzrJJHnCSGP0",
        "colab_type": "text"
      },
      "source": [
        "78.72 % yes <br>\n",
        "21.28 % no <br>\n",
        "000.0 % girl <br>\n",
        "000.0 % halloween <br>\n",
        "000.0 % left <br>"
      ]
    }
  ]
}